{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import time\n",
    "import itertools\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import random \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from flash_attn.bert_padding import pad_input\n",
    "from scfoundation import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideData():\n",
    "    def __init__(self, data_path, slide, lr_path, pad_value, pad_token):\n",
    "        self.data_path = data_path\n",
    "        self.slide = slide\n",
    "        self.lr_path = lr_path\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        adata = sc.read_h5ad(f'{self.data_path}/{self.slide}_Visium_deconv.h5ad')\n",
    "        \n",
    "        scfoundation_gene_df = pd.read_csv(f'{tokenizer_dir}/scfoundation_gene_df.csv')\n",
    "        scfoundation_gene_df.set_index('gene_ids', inplace=True)\n",
    "        total_gene_num = adata.shape[1]\n",
    "        adata = adata[:, adata.var_names.isin(scfoundation_gene_df.index)]\n",
    "        adata.var['gene_name'] = scfoundation_gene_df.loc[adata.var_names, 'gene_symbols'].values\n",
    "        seleted_gene_num = adata.shape[1]\n",
    "\n",
    "        print(\n",
    "            f\"match {seleted_gene_num}/{total_gene_num} genes \"\n",
    "            f\"in vocabulary of size 19264.\"\n",
    "        )\n",
    "\n",
    "        for celltype in adata.layers.keys():\n",
    "            adata.X = adata.layers[celltype]\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata.uns.pop('log1p')\n",
    "            adata.layers[celltype] = adata.X\n",
    "\n",
    "        celltype_proportion = adata.obsm['q05_cell_abundance_w_sf']\n",
    "        celltype_proportion.rename(columns=lambda x: x[23:], inplace=True)\n",
    "        celltype_proportion = celltype_proportion.div(celltype_proportion.sum(axis=1), axis=0)\n",
    "        celltype_proportion[celltype_proportion < 0.05] = 0\n",
    "        celltype_proportion = celltype_proportion.div(celltype_proportion.sum(axis=1), axis=0)\n",
    "\n",
    "        self.adata = adata\n",
    "        self.celltype_proportion = celltype_proportion\n",
    "\n",
    "    def get_sc_data(self):\n",
    "        barcode_list = []\n",
    "        gexpr_feature = []\n",
    "        celltypes_labels = []\n",
    "        for i in range(self.adata.shape[0]):\n",
    "            barcode = self.adata.obs.index[i]\n",
    "            ct_prop = self.celltype_proportion.iloc[i][self.celltype_proportion.iloc[i]>0]\n",
    "            cell_num = 0\n",
    "            for ct in ct_prop.index:\n",
    "                celltypes_labels.append(ct)\n",
    "                cell_num += 1\n",
    "                barcode_list.append(f'{barcode}_{cell_num}')\n",
    "                gexpr_feature.append(self.adata.layers[ct][i].A)\n",
    "        gexpr_feature = np.concatenate(gexpr_feature)\n",
    "\n",
    "        adata_sc = anndata.AnnData(X=gexpr_feature, obs=pd.DataFrame({'celltype': celltypes_labels}, index=barcode_list), var=pd.DataFrame({'gene_name': self.adata.var['gene_name'].values}, index=self.adata.var_names.values), obsm=None)\n",
    "        self.adata = adata_sc\n",
    "\n",
    "    def get_lr_pairs(self):\n",
    "        gene_list_df = pd.read_csv(f'{tokenizer_dir}/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "        gene_list_df.set_index('gene_name', inplace=True)\n",
    "\n",
    "        ligand_database = pd.read_csv(tokenizer_dir+'ligand_database.csv', header=0, index_col=0)\n",
    "        ligand_symbol = ligand_database[ligand_database.sum(1)>1].index.values\n",
    "        ligand_ids = gene_list_df.loc[ligand_symbol.tolist(),'index'].values\n",
    "\n",
    "        lr_df = pd.read_csv(self.lr_path, sep='\\t', header=0)\n",
    "        lr_df = lr_df[lr_df['ligand_ensembl_gene_id'].isin(self.adata.var_names) & lr_df['receptor_ensembl_gene_id'].isin(self.adata.var_names)]\n",
    "        lr_df['ligand_gene_symbol'] = self.adata.var.loc[lr_df['ligand_ensembl_gene_id'], 'gene_name'].values\n",
    "        lr_df['receptor_gene_symbol'] = self.adata.var.loc[lr_df['receptor_ensembl_gene_id'], 'gene_name'].values\n",
    "        lr_df['ligand_gene_id'] = gene_list_df.loc[lr_df['ligand_gene_symbol'].tolist(),'index'].values\n",
    "        lr_df['receptor_gene_id'] = gene_list_df.loc[lr_df['receptor_gene_symbol'].tolist(),'index'].values\n",
    "        lr_df = lr_df[lr_df['ligand_gene_id'].isin(ligand_ids)]\n",
    "        lr_pairs = list(zip(lr_df['ligand_gene_id'], lr_df['receptor_gene_id']))\n",
    "        \n",
    "        random.seed(0)\n",
    "        lr_df = lr_df.iloc[random.sample(range(lr_df.shape[0]), 150)]\n",
    "        ligand_ids = list(set(lr_df['ligand_gene_id']))\n",
    "        receptor_ids = list(set(lr_df['receptor_gene_id']))\n",
    "        ligand_receptor_ids = ligand_ids + receptor_ids\n",
    "        ligand_receptor_labels = [1]*len(ligand_ids) + [0]*len(receptor_ids)\n",
    "        \n",
    "        self.ligand_receptor_ids = np.array(ligand_receptor_ids)\n",
    "        self.ligand_receptor_labels = np.array(ligand_receptor_labels)\n",
    "        self.lr_pairs = set(lr_pairs)\n",
    "\n",
    "    def prepare_data(self, sample_num):\n",
    "        gene_list_df = pd.read_csv(f'{tokenizer_dir}/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "        gene_list = list(gene_list_df['gene_name'])\n",
    "        self.gene_ids = gene_list_df['index'].values\n",
    "\n",
    "        gexpr_feature = self.adata.X\n",
    "        idx = self.adata.obs_names.tolist()\n",
    "        col = self.adata.var.gene_name.tolist()\n",
    "        gexpr_feature = pd.DataFrame(gexpr_feature, index=idx, columns=col)\n",
    "        gexpr_feature, _ = load.main_gene_selection(gexpr_feature, gene_list)\n",
    "        S = gexpr_feature.sum(1)\n",
    "        T = S\n",
    "        TS = np.concatenate([[np.log10(T)],[np.log10(S)]],axis=0).T\n",
    "        data = np.concatenate([gexpr_feature,TS],axis=1)\n",
    "        self.data = data[:sample_num]\n",
    "        # self.data = self.data[np.where(self.adata.obs['celltype'].values=='myofibroblast cell')[0]]\n",
    "\n",
    "    def _pad_information_of_split_token(self, token_num):\n",
    "        max_token_num = token_num.max().item()\n",
    "        total_cell_num = token_num.size(0)\n",
    "        key_padding_mask = torch.zeros((total_cell_num, max_token_num), dtype=torch.bool)\n",
    "        for i,val in enumerate(token_num):\n",
    "            key_padding_mask[i, val:] = True\n",
    "        indices = (~key_padding_mask.view(-1)).nonzero(as_tuple=True)[0]\n",
    "        return indices, total_cell_num, max_token_num, key_padding_mask\n",
    "\n",
    "    def prepare_train_and_valid_data(self, train_index, valid_index):\n",
    "        ligand_receptor_ids_train = self.ligand_receptor_ids[train_index]\n",
    "        ligand_receptor_labels_train = self.ligand_receptor_labels[train_index]\n",
    "        ligand_receptor_ids_valid = self.ligand_receptor_ids[valid_index]\n",
    "        ligand_receptor_labels_valid = self.ligand_receptor_labels[valid_index]\n",
    "\n",
    "        ligand_ids_train = set(ligand_receptor_ids_train[np.where(ligand_receptor_labels_train==1)[0]])\n",
    "        receptor_ids_train = set(ligand_receptor_ids_train[np.where(ligand_receptor_labels_train==0)[0]])\n",
    "        all_pairs_train = set(itertools.product(ligand_ids_train, receptor_ids_train))\n",
    "        pos_lr_train = list(all_pairs_train.intersection(self.lr_pairs))\n",
    "        random.seed(1)\n",
    "        neg_lr_train = random.sample(sorted(all_pairs_train.difference(self.lr_pairs)), len(pos_lr_train))\n",
    "        lr_train = pos_lr_train + neg_lr_train\n",
    "        \n",
    "        print(f\"number of pos/neg lr pairs in train set: {len(pos_lr_train)} / {len(neg_lr_train)}\")\n",
    "        \n",
    "        ligand_ids_valid = set(ligand_receptor_ids_valid[np.where(ligand_receptor_labels_valid==1)[0]])\n",
    "        receptor_ids_valid = set(ligand_receptor_ids_valid[np.where(ligand_receptor_labels_valid==0)[0]])\n",
    "        all_pairs_valid = set(itertools.product(ligand_ids_valid, receptor_ids_valid))\n",
    "        pos_lr_valid = list(all_pairs_valid.intersection(self.lr_pairs))\n",
    "        random.seed(2)\n",
    "        neg_lr_valid = random.sample(sorted(all_pairs_valid.difference(self.lr_pairs)), len(pos_lr_valid))\n",
    "        lr_valid = pos_lr_valid + neg_lr_valid\n",
    "\n",
    "        print(f\"number of pos/neg lr pairs in valid set: {len(pos_lr_valid)} / {len(neg_lr_valid)}\")\n",
    "\n",
    "        samples_l_ids_train = [set(self.gene_ids[np.nonzero(d[:-2])[0]]).intersection(ligand_ids_train) for d in self.data]\n",
    "        samples_r_ids_train = [set(self.gene_ids[np.nonzero(d[:-2])[0]]).intersection(receptor_ids_train) for d in self.data]\n",
    "        train_index = [k for k in range(len(self.data)) if len(set(itertools.product(samples_l_ids_train[k], samples_r_ids_train[k])).intersection(set(lr_train))) > 0]\n",
    "        samples_l_ids_valid = [set(self.gene_ids[np.nonzero(d[:-2])[0]]).intersection(ligand_ids_valid) for d in self.data]\n",
    "        samples_r_ids_valid = [set(self.gene_ids[np.nonzero(d[:-2])[0]]).intersection(receptor_ids_valid) for d in self.data]\n",
    "        valid_index = [k for k in range(len(self.data)) if len(set(itertools.product(samples_l_ids_valid[k], samples_r_ids_valid[k])).intersection(set(lr_valid))) > 0]\n",
    "\n",
    "        train_data = [self.data[k] for k in train_index]\n",
    "        train_data = torch.from_numpy(np.array(train_data)).float()\n",
    "        train_data_gene_ids = torch.arange(train_data.shape[1]).repeat(train_data.shape[0], 1)\n",
    "        train_data_index = train_data != 0\n",
    "        train_values, train_padding = load.gatherData(train_data, train_data_index, self.pad_value)\n",
    "        train_gene_ids, _ = load.gatherData(train_data_gene_ids, train_data_index, self.pad_token)\n",
    "        train_center_l = torch.isin(train_gene_ids, torch.tensor(list(ligand_ids_train)))\n",
    "        train_center_r = torch.isin(train_gene_ids, torch.tensor(list(receptor_ids_train)))\n",
    "        \n",
    "        valid_data = [self.data[k] for k in valid_index]\n",
    "        valid_data = torch.from_numpy(np.array(valid_data)).float()\n",
    "        valid_data_gene_ids = torch.arange(valid_data.shape[1]).repeat(valid_data.shape[0], 1)\n",
    "        valid_data_index = valid_data != 0\n",
    "        valid_values, valid_padding = load.gatherData(valid_data, valid_data_index, self.pad_value)\n",
    "        valid_gene_ids, _ = load.gatherData(valid_data_gene_ids, valid_data_index, self.pad_token)\n",
    "        valid_center_l = torch.isin(valid_gene_ids, torch.tensor(list(ligand_ids_valid)))\n",
    "        valid_center_r = torch.isin(valid_gene_ids, torch.tensor(list(receptor_ids_valid)))\n",
    "        \n",
    "        ligand_ids_train = ligand_ids_train.union({self.pad_token})\n",
    "        receptor_ids_train = receptor_ids_train.union({self.pad_token})\n",
    "        all_pairs_train = set(itertools.product(ligand_ids_train, receptor_ids_train))\n",
    "        lr2label_train = dict(zip(all_pairs_train, [-100]*len(all_pairs_train)))\n",
    "        for lr in pos_lr_train:\n",
    "            lr2label_train[lr] = 1\n",
    "        for lr in neg_lr_train:\n",
    "            lr2label_train[lr] = 0\n",
    "\n",
    "        split_indices, total_cell_num, max_l_num, split_key_padding_mask = self._pad_information_of_split_token(train_center_l.sum(1))\n",
    "        l_ids_train = pad_input(train_gene_ids[train_center_l].unsqueeze(-1), split_indices, total_cell_num, max_l_num).squeeze(-1)\n",
    "        l_ids_train[split_key_padding_mask] = self.pad_token\n",
    "        split_indices, total_cell_num, max_r_num, split_key_padding_mask = self._pad_information_of_split_token(train_center_r.sum(1))\n",
    "        r_ids_train = pad_input(train_gene_ids[train_center_r].unsqueeze(-1), split_indices, total_cell_num, max_r_num).squeeze(-1)\n",
    "        r_ids_train[split_key_padding_mask] = self.pad_token\n",
    "        lr_pairs_train = [list(itertools.product(l_ids_train[i].tolist(), r_ids_train[i].tolist())) for i in range(total_cell_num)]\n",
    "        train_lr_labels = torch.tensor([[lr2label_train[lr] for lr in cell] for cell in lr_pairs_train])\n",
    "        train_data = {'values': train_values, 'padding': train_padding, 'gene_ids': train_gene_ids, 'ligand':train_center_l, 'receptor':train_center_r, 'lr_labels': train_lr_labels}\n",
    "        \n",
    "        ligand_ids_valid = ligand_ids_valid.union({self.pad_token})\n",
    "        receptor_ids_valid = receptor_ids_valid.union({self.pad_token})\n",
    "        all_pairs_valid = set(itertools.product(ligand_ids_valid, receptor_ids_valid))\n",
    "        lr2label_valid = dict(zip(all_pairs_valid, [-100]*len(all_pairs_valid)))\n",
    "        for lr in pos_lr_valid:\n",
    "            lr2label_valid[lr] = 1\n",
    "        for lr in neg_lr_valid:\n",
    "            lr2label_valid[lr] = 0\n",
    "\n",
    "        split_indices, total_cell_num, max_l_num, split_key_padding_mask = self._pad_information_of_split_token(valid_center_l.sum(1))\n",
    "        l_ids_valid = pad_input(valid_gene_ids[valid_center_l].unsqueeze(-1), split_indices, total_cell_num, max_l_num).squeeze(-1)\n",
    "        l_ids_valid[split_key_padding_mask] = self.pad_token\n",
    "        split_indices, total_cell_num, max_r_num, split_key_padding_mask = self._pad_information_of_split_token(valid_center_r.sum(1))\n",
    "        r_ids_valid = pad_input(valid_gene_ids[valid_center_r].unsqueeze(-1), split_indices, total_cell_num, max_r_num).squeeze(-1)\n",
    "        r_ids_valid[split_key_padding_mask] = self.pad_token\n",
    "        lr_pairs_valid = [list(itertools.product(l_ids_valid[i].tolist(), r_ids_valid[i].tolist())) for i in range(total_cell_num)]\n",
    "        valid_lr_labels = torch.tensor([[lr2label_valid[lr] for lr in cell] for cell in lr_pairs_valid])\n",
    "        valid_data = {'values': valid_values, 'padding': valid_padding, 'gene_ids': valid_gene_ids, 'ligand':valid_center_l, 'receptor':valid_center_r, 'lr_labels': valid_lr_labels}\n",
    "\n",
    "        print(\n",
    "            f\"train set number of samples: {train_gene_ids.shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {train_gene_ids.shape[1]}\"\n",
    "            f\"\\n\\t feature length of lr pairs: {train_lr_labels.shape[1]}\"\n",
    "            f\"\\n\\t number of pos/neg lr pairs: {(train_lr_labels==1).sum().item()} / {(train_lr_labels==0).sum().item()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"valid set number of samples: {valid_gene_ids.shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {valid_gene_ids.shape[1]}\"\n",
    "            f\"\\n\\t feature length of lr pairs: {valid_lr_labels.shape[1]}\"\n",
    "            f\"\\n\\t number of pos/neg lr pairs: {(valid_lr_labels==1).sum().item()} / {(valid_lr_labels==0).sum().item()}\"\n",
    "        )\n",
    "\n",
    "        return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scF_lrc(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            d_model: int,\n",
    "            n_lrc: int = 2,\n",
    "            nlayers_lrc: int = 3,\n",
    "    ):\n",
    "        super(scF_lrc, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "        self.encoder = scf_encoder\n",
    "        self.lrc_decoder = LRCDecoder(d_model, n_lrc, nlayers=nlayers_lrc)\n",
    "\n",
    "    def _pad_information_of_split_input(self, encoder_feature_lens, max_seqlen):\n",
    "        total_cell_num = encoder_feature_lens.size(0)\n",
    "        key_padding_mask = torch.zeros((total_cell_num, max_seqlen), dtype=torch.bool, device=encoder_feature_lens.device)\n",
    "        for i,val in enumerate(encoder_feature_lens):\n",
    "            key_padding_mask[i, val:] = True\n",
    "        indices = (~key_padding_mask.view(-1)).nonzero(as_tuple=True)[0]\n",
    "        return indices, total_cell_num, max_seqlen, key_padding_mask\n",
    "\n",
    "    def forward(self, gene_values, padding_label, gene_ids, ligand, receptor, max_l_seqlen, max_r_seqlen):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(gene_values, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "\n",
    "        split_src_indices, total_cell_num, max_seqlen, _ = self._pad_information_of_split_input(ligand.sum(1), max_l_seqlen)\n",
    "        x_ligand = pad_input(x[ligand], split_src_indices, total_cell_num, max_seqlen)\n",
    "        split_src_indices, total_cell_num, max_seqlen, _ = self._pad_information_of_split_input(receptor.sum(1), max_r_seqlen)\n",
    "        x_receptor = pad_input(x[receptor], split_src_indices, total_cell_num, max_seqlen)\n",
    "            \n",
    "        x_lr = [list(itertools.product(x_ligand[i].cpu(), x_receptor[i].cpu())) for i in range(total_cell_num)]\n",
    "        x_lr = torch.tensor([[torch.concat(lr).tolist() for lr in cell] for cell in x_lr], device=x.device)\n",
    "\n",
    "        output = self.lrc_decoder(x_lr)\n",
    "\n",
    "        return output\n",
    "\n",
    "class LRCDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for ligand-receptor pair classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_lr: int = 2,\n",
    "        nlayers: int = 3,\n",
    "        activation: callable = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # module list\n",
    "        d_model = 2*d_model\n",
    "        self._decoder = nn.ModuleList()\n",
    "        for i in range(nlayers - 1):\n",
    "            self._decoder.append(nn.Linear(d_model, d_model))\n",
    "            self._decoder.append(activation())\n",
    "            self._decoder.append(nn.LayerNorm(d_model))\n",
    "        self.out_layer = nn.Linear(d_model, n_lr)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embsize]\n",
    "        \"\"\"\n",
    "        for layer in self._decoder:\n",
    "            x = layer(x)\n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, train_data, valid_data, batch_size, max_batch) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "    amp = True\n",
    "    schedule_ratio = 0.9\n",
    "    schedule_interval = 1\n",
    "    log_interval = 10\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, eps=1e-4 if amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, schedule_interval, gamma=schedule_ratio\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    model.train()\n",
    "    total_lrc = 0.0\n",
    "    total_error = 0.0\n",
    "    \n",
    "    best_model = None\n",
    "    best_auc_value = 0\n",
    "    best_fpr = 0\n",
    "    best_tpr = 0\n",
    "\n",
    "    max_l_seqlen = train_data['ligand'].sum(1).max().item()\n",
    "    max_r_seqlen = train_data['receptor'].sum(1).max().item()\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_values = train_data['values']\n",
    "    train_padding = train_data['padding']\n",
    "    train_gene_ids = train_data['gene_ids']\n",
    "    train_ligand = train_data['ligand']\n",
    "    train_receptor = train_data['receptor']\n",
    "    train_lr_labels = train_data['lr_labels']\n",
    "\n",
    "    num_batches = np.ceil(len(train_values)/batch_size).astype(int)\n",
    "    for k in range(0, len(train_values), batch_size):\n",
    "        batch = int(k/batch_size+1)\n",
    "        if batch > max_batch:\n",
    "            break\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output = model(train_values[k:k+batch_size].to(device), \n",
    "                           train_padding[k:k+batch_size].to(device), \n",
    "                           train_gene_ids[k:k+batch_size].to(device),\n",
    "                           train_ligand[k:k+batch_size].to(device), \n",
    "                           train_receptor[k:k+batch_size].to(device), \n",
    "                           max_l_seqlen, \n",
    "                           max_r_seqlen)\n",
    "            \n",
    "            batch_train_lr_labels = train_lr_labels[k:k+batch_size].to(device)\n",
    "            batch_logits = output[torch.logical_or(batch_train_lr_labels==1, batch_train_lr_labels==0)]\n",
    "            batch_labels = batch_train_lr_labels[torch.logical_or(batch_train_lr_labels==1, batch_train_lr_labels==0)]\n",
    "            loss_lrc = criterion_cls(batch_logits, batch_labels)\n",
    "\n",
    "            error_rate_lrc = 1 - (\n",
    "                    (batch_logits.argmax(1) == batch_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / batch_labels.size(0)\n",
    "            \n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_lrc).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_lrc += loss_lrc.item()\n",
    "        total_error += error_rate_lrc\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            sec_per_batch = (time.time() - start_time) / log_interval\n",
    "            cur_lrc = total_lrc / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            print(f\"| Split {split} | \"\n",
    "                f\"{batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.8f} | sec/batch {sec_per_batch:5.1f} | \"\n",
    "                f\"lrc {cur_lrc:5.5f} | \"\n",
    "                f\"err {cur_error:1.5f} | \"\n",
    "            )\n",
    "            total_lrc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "        if batch % (10*log_interval) == 0 and batch > 0:\n",
    "            auc_value, fpr, tpr = evaluate(model, valid_data, batch_size)\n",
    "            if auc_value > best_auc_value:\n",
    "                best_auc_value = auc_value\n",
    "                best_fpr = fpr\n",
    "                best_tpr = tpr\n",
    "                best_model = copy.deepcopy(model)\n",
    "            model.train()\n",
    "            start_time = time.time()\n",
    "\n",
    "    return best_model, best_fpr, best_tpr\n",
    "\n",
    "def py_softmax(vector):\n",
    "\te = np.exp(vector)\n",
    "\treturn e / e.sum()\n",
    "\n",
    "def evaluate(model: nn.Module, valid_data, batch_size) -> None:\n",
    "    amp = True\n",
    "    \n",
    "    model.eval()\n",
    "    total_lrc = 0.0\n",
    "    total_error = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    max_l_seqlen = valid_data['ligand'].sum(1).max().item()\n",
    "    max_r_seqlen = valid_data['receptor'].sum(1).max().item()\n",
    "\n",
    "    valid_values = valid_data['values']\n",
    "    valid_padding = valid_data['padding']\n",
    "    valid_gene_ids = valid_data['gene_ids']\n",
    "    valid_ligand = valid_data['ligand']\n",
    "    valid_receptor = valid_data['receptor']\n",
    "    valid_lr_labels = valid_data['lr_labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in tqdm(range(0, len(valid_values), batch_size)):\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output = model(valid_values[k:k+batch_size].to(device), \n",
    "                           valid_padding[k:k+batch_size].to(device), \n",
    "                           valid_gene_ids[k:k+batch_size].to(device),\n",
    "                           valid_ligand[k:k+batch_size].to(device), \n",
    "                           valid_receptor[k:k+batch_size].to(device), \n",
    "                           max_l_seqlen, \n",
    "                           max_r_seqlen)\n",
    "\n",
    "                batch_valid_lr_labels = valid_lr_labels[k:k+batch_size].to(device)\n",
    "                batch_logits = output[torch.logical_or(batch_valid_lr_labels==1, batch_valid_lr_labels==0)]\n",
    "                batch_labels = batch_valid_lr_labels[torch.logical_or(batch_valid_lr_labels==1, batch_valid_lr_labels==0)]\n",
    "                \n",
    "                logits_list.append(batch_logits.cpu())\n",
    "                labels_list.append(batch_labels.cpu())\n",
    "            \n",
    "            accuracy = (batch_logits.argmax(1) == batch_labels).sum().item()\n",
    "            total_error += batch_labels.size(0) - accuracy\n",
    "            total_num += batch_labels.size(0)\n",
    "            total_lrc += criterion_cls(batch_logits, batch_labels).item()*batch_labels.size(0)\n",
    "\n",
    "    logits = torch.cat(logits_list)\n",
    "    labels = torch.cat(labels_list)\n",
    "\n",
    "    y_score = [py_softmax(item)[1] for item in logits.numpy()]\n",
    "    y_true = labels.numpy()\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "\n",
    "    val_err = total_error / total_num\n",
    "    val_loss = total_lrc / total_num\n",
    "    print(\"-\" * 89)\n",
    "    print(\n",
    "        f\"valid accuracy: {1-val_err:1.4f} | \"\n",
    "        f\"valid auc: {auc_value:1.4f} | \"\n",
    "        f\"valid loss {val_loss:1.4f} | \"\n",
    "        f\"valid err {val_err:1.4f}\"\n",
    "    )\n",
    "    print(\"-\" * 89)\n",
    "    \n",
    "    return auc_value, fpr, tpr\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_data, valid_data, batch_size, max_batch):\n",
    "\n",
    "    best_model, best_fpr, best_tpr = train(model, train_data, valid_data, batch_size, max_batch)\n",
    "    \n",
    "    return best_model, best_fpr, best_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scFoundation(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            scf_decoder,\n",
    "            scf_decoder_embed,\n",
    "            scf_norm,\n",
    "            scf_to_final,\n",
    "    ):\n",
    "        super(scFoundation, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "\n",
    "        # ## DEBUG\n",
    "        self.encoder = scf_encoder\n",
    "\n",
    "        ##### decoder\n",
    "        self.decoder = scf_decoder\n",
    "        self.decoder_embed = scf_decoder_embed\n",
    "        self.norm = scf_norm\n",
    "        self.to_final = scf_to_final\n",
    "\n",
    "    def forward(self, x, padding_label, encoder_position_gene_ids, encoder_labels, decoder_data,\n",
    "                decoder_position_gene_ids, decoder_data_padding_labels, **kwargs):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(x, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(encoder_position_gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "\n",
    "        decoder_data = self.token_emb(torch.unsqueeze(decoder_data, 2))\n",
    "        position_emb = self.pos_emb(decoder_position_gene_ids)\n",
    "        batch_idx, gen_idx = (encoder_labels == True).nonzero(as_tuple=True)\n",
    "        decoder_data[batch_idx, gen_idx] = x[~padding_label].to(decoder_data.dtype)\n",
    "\n",
    "        decoder_data += position_emb\n",
    "\n",
    "        decoder_data = self.decoder_embed(decoder_data)\n",
    "        x = self.decoder(decoder_data, padding_mask=decoder_data_padding_labels)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        # return x\n",
    "        x = self.to_final(x)\n",
    "        return x.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_file):\n",
    "    if model_file is None:\n",
    "        pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "    else:\n",
    "        pretrainmodel = torch.load(f'scfoundation/fine-tuning/{model_file}', map_location='cpu')\n",
    "        pretrainmodel = pretrainmodel.module\n",
    "\n",
    "    model = scF_lrc(pretrainmodel.token_emb,\n",
    "            pretrainmodel.pos_emb,\n",
    "            pretrainmodel.encoder,\n",
    "            d_model = 768,\n",
    "            n_lrc = 2,\n",
    "            nlayers_lrc = 3\n",
    "            )\n",
    "    \n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    for name, para in model.named_parameters():\n",
    "            para.requires_grad = False\n",
    "    for name, para in model.lrc_decoder.named_parameters():\n",
    "            para.requires_grad = True\n",
    "    post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    print(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "    print(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 19266\n",
    "pad_value = 103\n",
    "tokenizer_dir = '/home/shcao/spFormer/spformer/tokenizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = None # 'model_human_myocardial_infarction.ckpt'\n",
    "\n",
    "dataset = 'human_myocardial_infarction_dataset'\n",
    "slide = '10X001'\n",
    "data_path = f'../data/{dataset}/'\n",
    "lr_path = 'gene_lists/human_LR_pairs.txt'\n",
    "slideData = SlideData(data_path, slide, lr_path, pad_value, pad_token)\n",
    "slideData.get_sc_data()\n",
    "slideData.get_lr_pairs()\n",
    "\n",
    "sample_num = 5000\n",
    "slideData.prepare_data(sample_num)\n",
    "\n",
    "batch_size = 6\n",
    "max_batch = 500\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "all_tpr = []\n",
    "all_roc_auc = []\n",
    "all_tpr_wt = []\n",
    "\n",
    "split = 0\n",
    "for train_index, valid_index in skf.split(slideData.ligand_receptor_ids, slideData.ligand_receptor_labels):\n",
    "    split += 1\n",
    "    print(f\"Cross-validate on dataset {dataset} slide {slide} - split {split}\")\n",
    "    train_data, valid_data = slideData.prepare_train_and_valid_data(train_index, valid_index)\n",
    "    \n",
    "    model = initialize_model(model_file)\n",
    "    model = nn.DataParallel(model, device_ids = [1, 3, 0])\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_model, best_fpr, best_tpr = train_and_evaluate(model, train_data, valid_data, batch_size, max_batch)\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    interp_tpr = np.interp(mean_fpr, best_fpr, best_tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    all_tpr.append(interp_tpr)\n",
    "    all_roc_auc.append(auc(best_fpr, best_tpr))\n",
    "    all_tpr_wt.append(len(best_tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt):\n",
    "    wts = [count/sum(all_tpr_wt) for count in all_tpr_wt]\n",
    "    print(wts)\n",
    "\n",
    "    all_weighted_tpr = [a*b for a,b in zip(all_tpr, wts)]\n",
    "    mean_tpr = np.sum(all_weighted_tpr, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    median_tpr = np.median(all_tpr, axis=0)\n",
    "    median_tpr[-1] = 1.0\n",
    "\n",
    "    all_weighted_roc_auc = [a*b for a,b in zip(all_roc_auc, wts)]\n",
    "    roc_auc_mean = np.sum(all_weighted_roc_auc)\n",
    "    roc_auc_sd = math.sqrt(np.average((all_roc_auc-roc_auc_mean)**2, weights=wts))\n",
    "\n",
    "    roc_auc_median = auc(mean_fpr, median_tpr)\n",
    "\n",
    "    return mean_tpr, median_tpr, roc_auc_mean, roc_auc_sd, roc_auc_median, wts\n",
    "\n",
    "mean_tpr, median_tpr, roc_auc_mean, roc_auc_sd, roc_auc_median, wts = get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt)\n",
    "print(f\"Mean ROC AUC: {roc_auc_mean} +/- {roc_auc_sd}\")\n",
    "cv_results = {'roc_auc_mean':roc_auc_mean, 'roc_auc_sd':roc_auc_sd, 'roc_auc_median':roc_auc_median, 'mean_fpr':mean_fpr, 'mean_tpr':mean_tpr, 'median_tpr':median_tpr, 'all_roc_auc':all_roc_auc, 'wts':wts}\n",
    "\n",
    "pickle.dump(cv_results, open(f'roc_results/scf-pt_lrc_{slide}_random2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ROC(bundled_data, title):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    for roc_auc, roc_auc_sd, mean_fpr, mean_tpr, sample, color in bundled_data:\n",
    "        plt.plot(mean_fpr, mean_tpr, color=color,\n",
    "                 lw=lw, label=\"{0} (AUC {1:0.2f} $\\pm$ {2:0.2f})\".format(sample, roc_auc, roc_auc_sd))\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "bundled_data = [(roc_auc_mean, roc_auc_sd, mean_fpr, mean_tpr, \"scf-ft\", \"blue\")]\n",
    "\n",
    "plot_ROC(bundled_data, 'Gene classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
