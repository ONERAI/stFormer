{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from typing import Dict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from flash_attn.bert_padding import pad_input\n",
    "\n",
    "from stformer import logger\n",
    "from stformer.tokenizer import GeneVocab\n",
    "from stformer.tokenizer import tokenize_and_pad_batch\n",
    "from stformer.model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideData():\n",
    "    def __init__(self, data_path, slide, lr_path, vocab, pad_value, pad_token):\n",
    "        self.data_path = data_path\n",
    "        self.slide = slide\n",
    "        self.lr_path = lr_path\n",
    "        self.vocab = vocab\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        adata = sc.read_h5ad(f'{self.data_path}/{self.slide}_Visium_deconv.h5ad')\n",
    "\n",
    "        scfoundation_gene_df = pd.read_csv(f'{tokenizer_dir}/scfoundation_gene_df.csv')\n",
    "        scfoundation_gene_df.set_index('gene_ids', inplace=True)\n",
    "        total_gene_num = adata.shape[1]\n",
    "        adata = adata[:, adata.var_names.isin(scfoundation_gene_df.index)]\n",
    "        adata.var['gene_name'] = scfoundation_gene_df.loc[adata.var_names, 'gene_symbols'].values\n",
    "        seleted_gene_num = adata.shape[1]\n",
    "        genes = adata.var[\"gene_name\"].tolist()\n",
    "        gene_ids = np.array(self.vocab(genes), dtype=int)\n",
    "\n",
    "        logger.info(\n",
    "            f\"match {seleted_gene_num}/{total_gene_num} genes \"\n",
    "            f\"in vocabulary of size 19264.\"\n",
    "        )\n",
    "\n",
    "        ligand_database = pd.read_csv(tokenizer_dir+'ligand_database.csv', header=0, index_col=0)\n",
    "        ligand_symbol = ligand_database[ligand_database.sum(1)>1].index.values\n",
    "        ligand_ids = self.vocab(ligand_symbol.tolist())\n",
    "        adata = adata[(adata[:,adata.var['gene_name'].isin(ligand_symbol)].X.sum(1)>0).A.T[0],:]\n",
    "            \n",
    "        celltype_proportion = adata.obsm['q05_cell_abundance_w_sf']\n",
    "        celltype_proportion.rename(columns=lambda x: x[23:], inplace=True)\n",
    "        celltype_proportion = celltype_proportion.div(celltype_proportion.sum(axis=1), axis=0)\n",
    "        celltype_proportion[celltype_proportion < 0.05] = 0\n",
    "        celltype_proportion = celltype_proportion.div(celltype_proportion.sum(axis=1), axis=0)\n",
    "\n",
    "        for celltype in adata.layers.keys():\n",
    "            adata.X = adata.layers[celltype]\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata.uns.pop('log1p')\n",
    "            adata.layers[celltype] = adata.X\n",
    "\n",
    "        self.adata = adata\n",
    "        self.celltype_proportion = celltype_proportion\n",
    "        self.gene_ids = gene_ids\n",
    "        self.ligand_ids = ligand_ids\n",
    "    \n",
    "    def get_lr_pairs(self):\n",
    "        lr_df = pd.read_csv(self.lr_path, sep='\\t', header=0)\n",
    "        lr_df = lr_df[lr_df['ligand_ensembl_gene_id'].isin(self.adata.var_names) & lr_df['receptor_ensembl_gene_id'].isin(self.adata.var_names)]\n",
    "        lr_df['ligand_gene_symbol'] = self.adata.var.loc[lr_df['ligand_ensembl_gene_id'], 'gene_name'].values\n",
    "        lr_df['receptor_gene_symbol'] = self.adata.var.loc[lr_df['receptor_ensembl_gene_id'], 'gene_name'].values\n",
    "        lr_df['ligand_gene_id'] = self.vocab(lr_df['ligand_gene_symbol'].tolist())\n",
    "        lr_df['receptor_gene_id'] = self.vocab(lr_df['receptor_gene_symbol'].tolist())\n",
    "        lr_df = lr_df[lr_df['ligand_gene_id'].isin(self.ligand_ids)]\n",
    "        lr_pairs = list(zip(lr_df['ligand_gene_id'], lr_df['receptor_gene_id']))\n",
    "        \n",
    "        random.seed(0)\n",
    "        lr_df = lr_df.iloc[random.sample(range(lr_df.shape[0]), 300)]\n",
    "        ligand_ids = list(set(lr_df['ligand_gene_id']))\n",
    "        receptor_ids = list(set(lr_df['receptor_gene_id']))\n",
    "        ligand_receptor_ids = ligand_ids + receptor_ids\n",
    "        ligand_receptor_labels = [1]*len(ligand_ids) + [0]*len(receptor_ids)\n",
    "        \n",
    "        self.ligand_receptor_ids = np.array(ligand_receptor_ids)\n",
    "        self.ligand_receptor_labels = np.array(ligand_receptor_labels)\n",
    "        self.lr_pairs = set(lr_pairs)\n",
    "\n",
    "    def get_niche_samples(self, sample_num):\n",
    "        samples_expression = []\n",
    "        samples_ctprop = []\n",
    "        celltypes_labels = []\n",
    "        for i in range(self.adata.shape[0]):\n",
    "            ct_prop = self.celltype_proportion.iloc[i][self.celltype_proportion.iloc[i]>0]\n",
    "\n",
    "            niche_counts = np.concatenate([self.adata.layers[ct][i].A for ct in ct_prop.index])\n",
    "            niche_counts[:,~np.isin(self.gene_ids, self.ligand_ids)] = 0\n",
    "            niche_ctprop = ct_prop.values\n",
    "\n",
    "            for ct in ct_prop.index:\n",
    "                counts = self.adata.layers[ct][i].A\n",
    "                samples_expression.append(np.concatenate([counts, niche_counts],axis=0))\n",
    "                samples_ctprop.append(niche_ctprop)\n",
    "                celltypes_labels.append(ct)\n",
    "\n",
    "        self.expression = samples_expression[:sample_num]\n",
    "        self.ctprop = samples_ctprop[:sample_num]\n",
    "        self.celltypes = celltypes_labels[:sample_num]\n",
    "\n",
    "        # self.expression = np.array(self.expression)[np.where(np.array(self.celltypes)=='myofibroblast cell')[0]]\n",
    "        # self.ctprop = np.array(self.ctprop)[np.where(np.array(self.celltypes)=='myofibroblast cell')[0]]\n",
    "\n",
    "    def _pad_information_of_split_token(self, token_num):\n",
    "        max_token_num = token_num.max().item()\n",
    "        total_cell_num = token_num.size(0)\n",
    "        key_padding_mask = torch.zeros((total_cell_num, max_token_num), dtype=torch.bool)\n",
    "        for i,val in enumerate(token_num):\n",
    "            key_padding_mask[i, val:] = True\n",
    "        indices = (~key_padding_mask.view(-1)).nonzero(as_tuple=True)[0]\n",
    "        return indices, total_cell_num, max_token_num, key_padding_mask\n",
    "    \n",
    "    def tokenize_data(self, train_index, valid_index):\n",
    "        ligand_receptor_ids_train = self.ligand_receptor_ids[train_index]\n",
    "        ligand_receptor_labels_train = self.ligand_receptor_labels[train_index]\n",
    "        ligand_receptor_ids_valid = self.ligand_receptor_ids[valid_index]\n",
    "        ligand_receptor_labels_valid = self.ligand_receptor_labels[valid_index]\n",
    "\n",
    "        ligand_ids_train = set(ligand_receptor_ids_train[np.where(ligand_receptor_labels_train==1)[0]])\n",
    "        receptor_ids_train = set(ligand_receptor_ids_train[np.where(ligand_receptor_labels_train==0)[0]])\n",
    "        all_pairs_train = set(itertools.product(ligand_ids_train, receptor_ids_train))\n",
    "        pos_lr_train = list(all_pairs_train.intersection(self.lr_pairs))\n",
    "        random.seed(1)\n",
    "        neg_lr_train = random.sample(sorted(all_pairs_train.difference(self.lr_pairs)), len(pos_lr_train))\n",
    "        lr_train = pos_lr_train + neg_lr_train\n",
    "        \n",
    "        logger.info(f\"number of pos/neg lr pairs in train set: {len(pos_lr_train)} / {len(neg_lr_train)}\")\n",
    "        \n",
    "        ligand_ids_valid = set(ligand_receptor_ids_valid[np.where(ligand_receptor_labels_valid==1)[0]])\n",
    "        receptor_ids_valid = set(ligand_receptor_ids_valid[np.where(ligand_receptor_labels_valid==0)[0]])\n",
    "        all_pairs_valid = set(itertools.product(ligand_ids_valid, receptor_ids_valid))\n",
    "        pos_lr_valid = list(all_pairs_valid.intersection(self.lr_pairs))\n",
    "        random.seed(2)\n",
    "        neg_lr_valid = random.sample(sorted(all_pairs_valid.difference(self.lr_pairs)), len(pos_lr_valid))\n",
    "        lr_valid = pos_lr_valid + neg_lr_valid\n",
    "\n",
    "        logger.info(f\"number of pos/neg lr pairs in valid set: {len(pos_lr_valid)} / {len(neg_lr_valid)}\")\n",
    "        \n",
    "        samples_expression = self.expression\n",
    "        samples_l_ids_train = [set(self.gene_ids[np.nonzero(d[0])[0]]).intersection(ligand_ids_train) for d in samples_expression]\n",
    "        samples_r_ids_train = [set(self.gene_ids[np.nonzero(d[0])[0]]).intersection(receptor_ids_train) for d in samples_expression]\n",
    "        train_index = [k for k in range(len(samples_expression)) if len(set(itertools.product(samples_l_ids_train[k], samples_r_ids_train[k])).intersection(set(lr_train))) > 0]\n",
    "        samples_l_ids_valid = [set(self.gene_ids[np.nonzero(d[0])[0]]).intersection(ligand_ids_valid) for d in samples_expression]\n",
    "        samples_r_ids_valid = [set(self.gene_ids[np.nonzero(d[0])[0]]).intersection(receptor_ids_valid) for d in samples_expression]\n",
    "        valid_index = [k for k in range(len(samples_expression)) if len(set(itertools.product(samples_l_ids_valid[k], samples_r_ids_valid[k])).intersection(set(lr_valid))) > 0]\n",
    "\n",
    "        train_data = [samples_expression[k] for k in train_index]\n",
    "        train_ctprop = [self.ctprop[k] for k in train_index]\n",
    "        valid_data = [samples_expression[k] for k in valid_index]\n",
    "        valid_ctprop = [self.ctprop[k] for k in valid_index]\n",
    "        \n",
    "        max_seq_len = np.max(np.count_nonzero(self.adata.X.A, axis=1))+2\n",
    "        max_niche_cell_num = (self.celltype_proportion>0).sum(1).max()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_niche_cell_num = max_niche_cell_num\n",
    "\n",
    "        tokenized_train = tokenize_and_pad_batch(\n",
    "            train_data,\n",
    "            train_ctprop,\n",
    "            self.gene_ids,\n",
    "            max_len = max_seq_len,\n",
    "            max_niche_cell_num = max_niche_cell_num,\n",
    "            vocab = self.vocab,\n",
    "            pad_token = self.pad_token,\n",
    "            pad_value = self.pad_value,\n",
    "            append_cls = False,  # append <cls> token at the beginning\n",
    "            include_zero_gene = False,\n",
    "        )\n",
    "\n",
    "        tokenized_valid = tokenize_and_pad_batch(\n",
    "            valid_data,\n",
    "            valid_ctprop,\n",
    "            self.gene_ids,\n",
    "            max_len = max_seq_len,\n",
    "            max_niche_cell_num = max_niche_cell_num,\n",
    "            vocab = self.vocab,\n",
    "            pad_token = self.pad_token,\n",
    "            pad_value = self.pad_value,\n",
    "            append_cls = False,\n",
    "            include_zero_gene = False,\n",
    "        )\n",
    "\n",
    "        tokenized_train['niche_l'] = torch.isin(tokenized_train['niche_genes'],torch.tensor(list(ligand_ids_train)))\n",
    "        tokenized_valid['niche_l'] = torch.isin(tokenized_valid['niche_genes'],torch.tensor(list(ligand_ids_valid)))\n",
    "        tokenized_train['center_l'] = torch.isin(tokenized_train['center_genes'],torch.tensor(list(ligand_ids_train)))\n",
    "        tokenized_valid['center_l'] = torch.isin(tokenized_valid['center_genes'],torch.tensor(list(ligand_ids_valid)))\n",
    "        tokenized_train['center_r'] = torch.isin(tokenized_train['center_genes'],torch.tensor(list(receptor_ids_train)))\n",
    "        tokenized_valid['center_r'] = torch.isin(tokenized_valid['center_genes'],torch.tensor(list(receptor_ids_valid)))\n",
    "        \n",
    "        ligand_ids_train = ligand_ids_train.union({self.vocab[self.pad_token]})\n",
    "        receptor_ids_train = receptor_ids_train.union({self.vocab[self.pad_token]})\n",
    "        all_pairs_train = set(itertools.product(ligand_ids_train, receptor_ids_train))\n",
    "        lr2label_train = dict(zip(all_pairs_train, [-100]*len(all_pairs_train)))\n",
    "        for lr in pos_lr_train:\n",
    "            lr2label_train[lr] = 1\n",
    "        for lr in neg_lr_train:\n",
    "            lr2label_train[lr] = 0\n",
    "\n",
    "        ligand_ids_valid = ligand_ids_valid.union({self.vocab[self.pad_token]})\n",
    "        receptor_ids_valid = receptor_ids_valid.union({self.vocab[self.pad_token]})\n",
    "        all_pairs_valid = set(itertools.product(ligand_ids_valid, receptor_ids_valid))\n",
    "        lr2label_valid = dict(zip(all_pairs_valid, [-100]*len(all_pairs_valid)))\n",
    "        for lr in pos_lr_valid:\n",
    "            lr2label_valid[lr] = 1\n",
    "        for lr in neg_lr_valid:\n",
    "            lr2label_valid[lr] = 0\n",
    "\n",
    "        split_indices, total_cell_num, max_l_num, split_key_padding_mask = self._pad_information_of_split_token(tokenized_train['center_l'].sum(1))\n",
    "        l_ids_train = pad_input(tokenized_train['center_genes'][tokenized_train['center_l']].unsqueeze(-1), split_indices, total_cell_num, max_l_num).squeeze(-1)\n",
    "        l_ids_train[split_key_padding_mask] = self.vocab[self.pad_token]\n",
    "        split_indices, total_cell_num, max_r_num, split_key_padding_mask = self._pad_information_of_split_token(tokenized_train['center_r'].sum(1))\n",
    "        r_ids_train = pad_input(tokenized_train['center_genes'][tokenized_train['center_r']].unsqueeze(-1), split_indices, total_cell_num, max_r_num).squeeze(-1)\n",
    "        r_ids_train[split_key_padding_mask] = self.vocab[self.pad_token]\n",
    "        lr_pairs_train = [list(itertools.product(l_ids_train[i].tolist(), r_ids_train[i].tolist())) for i in range(total_cell_num)]\n",
    "        tokenized_train['lr_labels'] = torch.tensor([[lr2label_train[lr] for lr in cell] for cell in lr_pairs_train])\n",
    "        \n",
    "        split_indices, total_cell_num, max_l_num, split_key_padding_mask = self._pad_information_of_split_token(tokenized_valid['center_l'].sum(1))\n",
    "        l_ids_valid = pad_input(tokenized_valid['center_genes'][tokenized_valid['center_l']].unsqueeze(-1), split_indices, total_cell_num, max_l_num).squeeze(-1)\n",
    "        l_ids_valid[split_key_padding_mask] = self.vocab[self.pad_token]\n",
    "        split_indices, total_cell_num, max_r_num, split_key_padding_mask = self._pad_information_of_split_token(tokenized_valid['center_r'].sum(1))\n",
    "        r_ids_valid = pad_input(tokenized_valid['center_genes'][tokenized_valid['center_r']].unsqueeze(-1), split_indices, total_cell_num, max_r_num).squeeze(-1)\n",
    "        r_ids_valid[split_key_padding_mask] = self.vocab[self.pad_token]\n",
    "        lr_pairs_valid = [list(itertools.product(l_ids_valid[i].tolist(), r_ids_valid[i].tolist())) for i in range(total_cell_num)]\n",
    "        tokenized_valid['lr_labels'] = torch.tensor([[lr2label_valid[lr] for lr in cell] for cell in lr_pairs_valid])\n",
    "        \n",
    "        logger.info(\n",
    "            f\"train set number of samples: {tokenized_train['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_train['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_train['niche_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of lr pairs: {tokenized_train['lr_labels'].shape[1]}\"\n",
    "            f\"\\n\\t number of pos/neg lr pairs: {(tokenized_train['lr_labels']==1).sum().item()} / {(tokenized_train['lr_labels']==0).sum().item()}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"valid set number of samples: {tokenized_valid['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_valid['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_valid['niche_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of lr pairs: {tokenized_valid['lr_labels'].shape[1]}\"\n",
    "            f\"\\n\\t number of pos/neg lr pairs: {(tokenized_valid['lr_labels']==1).sum().item()} / {(tokenized_valid['lr_labels']==0).sum().item()}\"\n",
    "        )\n",
    "\n",
    "        self.tokenized_train = tokenized_train\n",
    "        self.tokenized_valid = tokenized_valid\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_train[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_train[\"center_values\"],\n",
    "            \"target_center_values\": self.tokenized_train[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_train[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_train[\"niche_values\"],\n",
    "            \"niche_feature_lens\": self.tokenized_train[\"niche_feature_lens\"],\n",
    "            \"cross_attn_bias\": self.tokenized_train[\"cross_attn_bias\"],\n",
    "            \"niche_l\": self.tokenized_train[\"niche_l\"],\n",
    "            \"center_l\": self.tokenized_train[\"center_l\"],\n",
    "            \"center_r\": self.tokenized_train[\"center_r\"],\n",
    "            \"lr_labels\": self.tokenized_train[\"lr_labels\"],\n",
    "        }\n",
    "\n",
    "        self.valid_data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_valid[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_valid[\"center_values\"],\n",
    "            \"target_center_values\": self.tokenized_valid[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_valid[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_valid[\"niche_values\"],\n",
    "            \"niche_feature_lens\": self.tokenized_valid[\"niche_feature_lens\"],\n",
    "            \"cross_attn_bias\": self.tokenized_valid[\"cross_attn_bias\"],\n",
    "            \"niche_l\": self.tokenized_valid[\"niche_l\"],\n",
    "            \"center_l\": self.tokenized_valid[\"center_l\"],\n",
    "            \"center_r\": self.tokenized_valid[\"center_r\"],\n",
    "            \"lr_labels\": self.tokenized_valid[\"lr_labels\"],\n",
    "        }\n",
    "    \n",
    "    def prepare_dataloader(self, batch_size):\n",
    "        train_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.train_data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.valid_data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"center_gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, valid_loader, max_batch) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "    amp = True\n",
    "    schedule_ratio = 0.9\n",
    "    schedule_interval = 1\n",
    "    log_interval = 10\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, eps=1e-4 if amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, schedule_interval, gamma=schedule_ratio\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    model.train()\n",
    "    total_lrc = 0.0\n",
    "    total_error = 0.0\n",
    "\n",
    "    best_model = None\n",
    "    best_auc_value = 0\n",
    "    best_fpr = 0\n",
    "    best_tpr = 0\n",
    "\n",
    "    max_l_seqlen = slideData.tokenized_train['center_l'].sum(1).max().item()\n",
    "    max_r_seqlen = slideData.tokenized_train['center_r'].sum(1).max().item()\n",
    " \n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        if batch > max_batch:\n",
    "            break\n",
    "        niche_feature_lens = batch_data[\"niche_feature_lens\"].to(device)\n",
    "        # if niche_feature_lens.size(0)<7:\n",
    "        #     continue\n",
    "        center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "        input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "        niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "        input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "        cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "        \n",
    "        niche_l = batch_data[\"niche_l\"].to(device)\n",
    "        center_l = batch_data[\"center_l\"].to(device)\n",
    "        center_r = batch_data[\"center_r\"].to(device)\n",
    "        lr_labels = batch_data[\"lr_labels\"].to(device)\n",
    "        \n",
    "        encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "        decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                    niche_gene_ids,\n",
    "                    input_niche_values,\n",
    "                    encoder_src_key_padding_mask,\n",
    "                    center_gene_ids,\n",
    "                    input_center_values,\n",
    "                    decoder_src_key_padding_mask,\n",
    "                    cross_attn_bias,\n",
    "                    niche_l = niche_l,\n",
    "                    center_l = center_l,\n",
    "                    center_r = center_r,\n",
    "                    max_l_seqlen = max_l_seqlen,\n",
    "                    max_r_seqlen = max_r_seqlen,\n",
    "                    LRC = True,\n",
    "                )\n",
    "    \n",
    "            lrc_output = output_dict[\"lrc_output\"]\n",
    "            batch_logits = lrc_output[torch.logical_or(lr_labels==1, lr_labels==0)]\n",
    "            batch_labels = lr_labels[torch.logical_or(lr_labels==1, lr_labels==0)]\n",
    "            loss_lrc = criterion_cls(batch_logits, batch_labels)\n",
    "\n",
    "            error_rate_lrc = 1 - (\n",
    "                    (batch_logits.argmax(1) == batch_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / batch_labels.size(0)\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_lrc).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_lrc += loss_lrc.item()\n",
    "        total_error += error_rate_lrc\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            sec_per_batch = (time.time() - start_time) / log_interval\n",
    "            cur_lrc = total_lrc / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            logger.info(\n",
    "                f\"| Split {split} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.8f} | sec/batch {sec_per_batch:5.1f} | \"\n",
    "                f\"gcl {cur_lrc:5.5f} | \"\n",
    "                f\"err {cur_error:1.5f} | \"\n",
    "            )\n",
    "            total_lrc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()       \n",
    "        if batch % (10*log_interval) == 0 and batch > 0:\n",
    "            auc_value, fpr, tpr = evaluate(model, valid_loader)\n",
    "            if auc_value > best_auc_value:\n",
    "                best_auc_value = auc_value\n",
    "                best_fpr = fpr\n",
    "                best_tpr = tpr\n",
    "                best_model = copy.deepcopy(model)\n",
    "            model.train()\n",
    "            start_time = time.time()\n",
    "    \n",
    "    return best_model, best_fpr, best_tpr\n",
    "\n",
    "def py_softmax(vector):\n",
    "\te = np.exp(vector)\n",
    "\treturn e / e.sum()\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "    \n",
    "    model.eval()\n",
    "    total_lrc = 0.0\n",
    "    total_error = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    max_l_seqlen = slideData.tokenized_valid['center_l'].sum(1).max().item()\n",
    "    max_r_seqlen = slideData.tokenized_valid['center_r'].sum(1).max().item()\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    \n",
    "    batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader):\n",
    "            batch_num += 1\n",
    "            # if batch_num>100:\n",
    "            #     break\n",
    "            niche_feature_lens = batch_data[\"niche_feature_lens\"].to(device)\n",
    "            # if niche_feature_lens.size(0)<7:\n",
    "            #     continue\n",
    "            center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "            input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "            niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "            input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "            cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "\n",
    "            niche_l = batch_data[\"niche_l\"].to(device)\n",
    "            center_l = batch_data[\"center_l\"].to(device)\n",
    "            center_r = batch_data[\"center_r\"].to(device)\n",
    "            lr_labels = batch_data[\"lr_labels\"].to(device)\n",
    "\n",
    "            encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "            decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                        niche_gene_ids,\n",
    "                        input_niche_values,\n",
    "                        encoder_src_key_padding_mask,\n",
    "                        center_gene_ids,\n",
    "                        input_center_values,\n",
    "                        decoder_src_key_padding_mask,\n",
    "                        cross_attn_bias,\n",
    "                        niche_l = niche_l,\n",
    "                        center_l = center_l,\n",
    "                        center_r = center_r,\n",
    "                        max_l_seqlen = max_l_seqlen,\n",
    "                        max_r_seqlen = max_r_seqlen,\n",
    "                        LRC = True,\n",
    "                    )\n",
    "                lrc_output = output_dict[\"lrc_output\"]\n",
    "                batch_logits = lrc_output[torch.logical_or(lr_labels==1, lr_labels==0)]\n",
    "                batch_labels = lr_labels[torch.logical_or(lr_labels==1, lr_labels==0)]\n",
    "                logits.append(batch_logits.to('cpu'))\n",
    "                labels.append(batch_labels.to('cpu'))\n",
    "\n",
    "            accuracy = (batch_logits.argmax(1) == batch_labels).sum().item()\n",
    "            total_error += batch_labels.size(0) - accuracy\n",
    "            total_num += batch_labels.size(0)\n",
    "            total_lrc += criterion_cls(batch_logits, batch_labels).item()*batch_labels.size(0)\n",
    "\n",
    "    logits = torch.cat(logits)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    y_score = [py_softmax(item)[1] for item in logits.numpy()]\n",
    "    y_true = labels.numpy()\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    \n",
    "    val_err = total_error / total_num\n",
    "    val_loss = total_lrc / total_num\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"valid accuracy: {1-val_err:1.4f} | \"\n",
    "        f\"valid auc: {auc_value:1.4f} | \"\n",
    "        f\"valid loss {val_loss:1.4f} | \"\n",
    "        f\"valid err {val_err:1.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    return auc_value, fpr, tpr        \n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, max_batch):\n",
    "\n",
    "    best_model, best_fpr, best_tpr = train(model, train_loader, valid_loader, max_batch)\n",
    "\n",
    "    return best_model, best_fpr, best_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scfoundation import load\n",
    "\n",
    "def initialize_model(model_file):\n",
    "    pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "    \n",
    "    model = TransformerModel(\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        do_lrc = True,\n",
    "        nlayers_lrc = 3,\n",
    "        n_lrc = 2,\n",
    "        dropout = dropout,\n",
    "        cell_emb_style = cell_emb_style,\n",
    "        scfoundation_token_emb1 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "        scfoundation_token_emb2 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "        scfoundation_pos_emb1 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "        scfoundation_pos_emb2 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "    )\n",
    "\n",
    "    pt_model = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = pt_model.state_dict()\n",
    "    pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if 'lrc_decoder' not in k and 'gcl_decoder' not in k\n",
    "                # if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    # for k, v in pretrained_dict.items():\n",
    "    #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "    for name, para in model.lrc_decoder.named_parameters():\n",
    "        para.requires_grad = True\n",
    "    post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "    logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "    logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embsize = 768 #256\n",
    "d_hid = 3072 #1024\n",
    "nhead = 12 #4\n",
    "nlayers = 6 #12\n",
    "dropout = 0.1\n",
    "cell_emb_style = 'max-pool'\n",
    "LRC = True\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "pad_value = 103\n",
    "tokenizer_dir = '../stformer/tokenizer/'\n",
    "vocab_file = tokenizer_dir + \"scfoundation_gene_vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "vocab.append_token(pad_token)\n",
    "vocab.set_default_index(vocab[pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '../pretraining/model.ckpt'\n",
    "\n",
    "dataset = 'human_myocardial_infarction_dataset'\n",
    "slide = '10X001'\n",
    "data_path = f'../data/{dataset}/'\n",
    "lr_path = 'gene_lists/human_LR_pairs.txt'\n",
    "slideData = SlideData(data_path, slide, lr_path, vocab, pad_value, pad_token)\n",
    "slideData.get_lr_pairs()\n",
    "\n",
    "sample_num = 5000\n",
    "slideData.get_niche_samples(sample_num)\n",
    "\n",
    "batch_size = 6\n",
    "max_batch = 500\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "all_tpr = []\n",
    "all_roc_auc = []\n",
    "all_tpr_wt = []\n",
    "\n",
    "split = 0\n",
    "for train_index, valid_index in skf.split(slideData.ligand_receptor_ids, slideData.ligand_receptor_labels):\n",
    "    split += 1\n",
    "    logger.info(f\"Cross-validate on dataset {dataset} slide {slide} - split {split}\")\n",
    "    slideData.tokenize_data(train_index, valid_index)\n",
    "    slideData.prepare_data()\n",
    "    train_loader, valid_loader = slideData.prepare_dataloader(batch_size)\n",
    "\n",
    "    model = initialize_model(model_file)\n",
    "    model = nn.DataParallel(model, device_ids = [0, 3, 1])\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_model, best_fpr, best_tpr = train_and_evaluate(model, train_loader, valid_loader, max_batch)\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    interp_tpr = np.interp(mean_fpr, best_fpr, best_tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    all_tpr.append(interp_tpr)\n",
    "    all_roc_auc.append(auc(best_fpr, best_tpr))\n",
    "    all_tpr_wt.append(len(best_tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt):\n",
    "    wts = [count/sum(all_tpr_wt) for count in all_tpr_wt]\n",
    "    print(wts)\n",
    "\n",
    "    all_weighted_tpr = [a*b for a,b in zip(all_tpr, wts)]\n",
    "    mean_tpr = np.sum(all_weighted_tpr, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    median_tpr = np.median(all_tpr, axis=0)\n",
    "    median_tpr[-1] = 1.0\n",
    "\n",
    "    all_weighted_roc_auc = [a*b for a,b in zip(all_roc_auc, wts)]\n",
    "    roc_auc_mean = np.sum(all_weighted_roc_auc)\n",
    "    roc_auc_sd = math.sqrt(np.average((all_roc_auc-roc_auc_mean)**2, weights=wts))\n",
    "\n",
    "    roc_auc_median = auc(mean_fpr, median_tpr)\n",
    "\n",
    "    return mean_tpr, median_tpr, roc_auc_mean, roc_auc_sd, roc_auc_median, wts\n",
    "\n",
    "mean_tpr, median_tpr, roc_auc_mean, roc_auc_sd, roc_auc_median, wts = get_cross_valid_metrics(all_tpr, all_roc_auc, all_tpr_wt)\n",
    "\n",
    "print(f\"Mean ROC AUC: {roc_auc_mean} +/- {roc_auc_sd}\")\n",
    "cv_results = {'roc_auc_mean':roc_auc_mean, 'roc_auc_sd':roc_auc_sd, 'roc_auc_median':roc_auc_median, 'mean_fpr':mean_fpr, 'mean_tpr':mean_tpr, 'median_tpr':median_tpr, 'all_roc_auc':all_roc_auc, 'wts':wts}\n",
    "pickle.dump(cv_results, open(f'roc_results/stformer_lrc_{slide}_random2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ROC(bundled_data, title):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    for roc_auc, roc_auc_sd, mean_fpr, mean_tpr, sample, color in bundled_data:\n",
    "        plt.plot(mean_fpr, mean_tpr, color=color,\n",
    "                 lw=lw, label=\"{0} (AUC {1:0.2f} $\\pm$ {2:0.2f})\".format(sample, roc_auc, roc_auc_sd))\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "bundled_data = [(roc_auc_mean, roc_auc_sd, mean_fpr, mean_tpr, \"stFormer\", \"red\")]\n",
    "\n",
    "plot_ROC(bundled_data, 'Gene classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
